{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neibour\n",
    "Lazy learning methods: similar instances belong to same classifiers.\n",
    "-   need similar measure and training set. \n",
    "\n",
    "K-NN: not only look the most similar case, look for k most similar casse\n",
    "- similar case\n",
    "- Voronoi space-> separate space for each class. \n",
    "-  Distance beetwen points is Euclidean distance.\n",
    "    -  CAVEAT: each of columns have to be in same range of values. \n",
    "\n",
    "Problems: \n",
    "- Testing: have to campare to all examples. \n",
    "- There is noise. \n",
    "- If we include columns that are not relevant, something in the computation will be random (need feature selection).\n",
    "\n",
    "Parameters k-NN algorithm: \n",
    "- Hyperparameter: k, need to trained. \n",
    "- Training set\n",
    "- distance mesure \n",
    "\n",
    "Select k: \n",
    "- trying, cross-validation \n",
    "- We have training data set & Testing data set (validation)\n",
    "- create graph, wich X is the values of K I want try, and Y is the ACCuracy. You will get a curve, and you select the K with high ACC.\n",
    "- found k, create a model with the training data set and validated it with Testing data set. \n",
    "- Will high K, we will lost locarity. \n",
    "- Define <b> weigth </b> for each k-closted example. Know the weitgh, use the sign function to select the most closted (Kernel functions, try which is the best approach to find the weight)\n",
    "- Feature selection: use weighted distance. \n",
    " \n",
    "## Naive Bayes (probabilistic model)\n",
    "- Give a points (a vector). \n",
    "- Given the class, I can stimate the probability of the points. \n",
    "- I want to know, given my data, which is the probability it belong to class C. \n",
    "- Bayes theorem\n",
    "\n",
    "Computing probability\n",
    "- I have m positive case, and n negative case. -> p(Cn) = #positive cases/ #total\n",
    "\n",
    "- p (x1..xn) = #time that vector appear in positive case / # all positive cases\n",
    "    - It is no practical. We need to estimate this probability. \n",
    "    - Solution: Each column is independent with each other. No is fullfilled but we will assum, because it makes P(x1..xn|Cj) easy to compute\n",
    "    - P(x1..xn|Cj) = P(x1|Cj) * ... * P(xn|Cj)\n",
    "    - Cnb = arg max P(Cj) * Productory(p (Xi| Cj)).\n",
    "    - practical: use log when the values get smaller\n",
    "\n",
    "\n",
    "Represent documents\n",
    "- Transform the document to bag of words, we only know if a word is in the document. \n",
    "    - matrix: rows is documents, colums is the words, values-> if the document contain the word. \n",
    "    - Reduce words number: \n",
    "        - singural/plural, stemming, stop words\n",
    "    - How to compute: slide 29, 30\n",
    "    - Problem: rare wolds, the probability can be 0, this will make Vbn=0. \n",
    "    - Sol.: Laplace smoothing: a word can be more smaller but never 0. Add some values in the numerator and denominator. \n",
    "    \n",
    "- Gaussian equacions, normal distribution. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
