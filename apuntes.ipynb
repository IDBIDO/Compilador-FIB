{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neibour\n",
    "Lazy learning methods: similar instances belong to same classifiers.\n",
    "-   need similar measure and training set. \n",
    "\n",
    "K-NN: not only look the most similar case, look for k most similar casse\n",
    "- similar case\n",
    "- Voronoi space-> separate space for each class. \n",
    "-  Distance beetwen points is Euclidean distance.\n",
    "    -  CAVEAT: each of columns have to be in same range of values. \n",
    "\n",
    "Problems: \n",
    "- Testing: have to campare to all examples. \n",
    "- There is noise. \n",
    "- If we include columns that are not relevant, something in the computation will be random (need feature selection).\n",
    "\n",
    "Parameters k-NN algorithm: \n",
    "- Hyperparameter: k, need to trained. \n",
    "- Training set\n",
    "- distance mesure \n",
    "\n",
    "Select k: \n",
    "- trying, cross-validation \n",
    "- We have training data set & Testing data set (validation)\n",
    "- create graph, wich X is the values of K I want try, and Y is the ACCuracy. You will get a curve, and you select the K with high ACC.\n",
    "- found k, create a model with the training data set and validated it with Testing data set. \n",
    "- Will high K, we will lost locarity. \n",
    "- Define <b> weigth </b> for each k-closted example. Know the weitgh, use the sign function to select the most closted (Kernel functions, try which is the best approach to find the weight)\n",
    "- Feature selection: use weighted distance. \n",
    " \n",
    "## Naive Bayes (probabilistic model)\n",
    "- Give a points (a vector). \n",
    "- Given the class, I can stimate the probability of the points. \n",
    "- I want to know, given my data, which is the probability it belong to class C. \n",
    "- Bayes theorem\n",
    "\n",
    "Computing probability\n",
    "- I have m positive case, and n negative case. -> p(Cn) = #positive cases/ #total\n",
    "\n",
    "- p (x1..xn) = #time that vector appear in positive case / # all positive cases\n",
    "    - It is no practical. We need to estimate this probability. \n",
    "    - Solution: Each column is independent with each other. No is fullfilled but we will assum, because it makes P(x1..xn|Cj) easy to compute\n",
    "    - P(x1..xn|Cj) = P(x1|Cj) * ... * P(xn|Cj)\n",
    "    - Cnb = arg max P(Cj) * Productory(p (Xi| Cj)).\n",
    "    - practical: use log when the values get smaller\n",
    "\n",
    "\n",
    "Represent documents\n",
    "- Transform the document to bag of words, we only know if a word is in the document. \n",
    "    - matrix: rows is documents, colums is the words, values-> if the document contain the word. \n",
    "    - Reduce words number: \n",
    "        - singural/plural, stemming, stop words\n",
    "    - How to compute: slide 29, 30\n",
    "    - Problem: rare wolds, the probability can be 0, this will make Vbn=0. \n",
    "    - Sol.: Laplace smoothing: a word can be more smaller but never 0. Add some values in the numerator and denominator. \n",
    "    \n",
    "- Gaussian equacions, normal distribution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADB\n",
    "\n",
    "max (2d+1)^h-1 * 2d\n",
    "min 2*(d+1)^h-2 *d\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estructura\n",
    "- Data description\n",
    "- Data preprocessing\n",
    "   * Feature selection\n",
    "      - correlation \n",
    "      - information gain\n",
    "      - fisher score\n",
    "      - results\n",
    "   * One hot encoding \n",
    "   * Variance threshold \n",
    "   * Mean encoding \n",
    "   * Balancing \n",
    "   * Preprocessing results\n",
    "- Evaluation criteria\n",
    "- Machine learning algorithm\n",
    "    * Naive Bayes\n",
    "    * KNN\n",
    "    ...7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decicion tree \n",
    "node of tree correspont to the atributes in the data set \n",
    "for each node we have braches represent the diferent posibilities of the atributes\n",
    "when arrives a left of the tree, we get the label. \n",
    "Ex: Sunny -> Humidity -> High -> No play tennis \n",
    "\n",
    "> DT can be represent complex rules. Ex: (outlook = sunny ^ humidity = high) v (outlook = overcast) => play tennis\n",
    "\n",
    "> Don't need to normalize the data, because the tree is not affected by the scale of the data.\n",
    "\n",
    "> DT are not affected by outliers\n",
    "\n",
    "> DT are not affected by the scale of the data\n",
    "\n",
    "> DT don't need feature selection\n",
    "\n",
    "Q: What is the difference between a decision tree and a decision stump?\n",
    "A: Decision stump is a decision tree with only one node\n",
    "\n",
    "\n",
    "### Metode \n",
    "- ID3 \n",
    "- outlook: training data set \n",
    "- create branch from outook. Classify the data set in the new branch. Ex: Outlook -> sunny | overcast | rain\n",
    "\n",
    "### Entropy\n",
    "- measure of the impurity of a set of examples.\n",
    "- Formula: \n",
    "    - H(S) = -sum (p(i) * log2(p(i)))\n",
    "    - p(i) = #examples of class i / #total examples\n",
    "    - H(S) = 0 -> all examples are the same class\n",
    "    - H(S) = 1 -> all examples are different classes\n",
    "    - H(S) = 0.5 -> half of the examples are one class, the other half are the other class\n",
    "\n",
    "\n",
    "\n",
    "- overfitting: the tree is too complex, and it is not able to generalize.\n",
    "- aboid overfitting: \n",
    "    - stop growing the tree when the entropy is 0\n",
    "    - stop growing the tree when the entropy is too small \n",
    "    - put limit in the information gain, put only if I have more than the minimun amount of information. \n",
    "    - minumum number of examples in a leaf\n",
    "    - max depth of the tree\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
